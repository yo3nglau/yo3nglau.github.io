<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural-Symbolic on yo3nglau academic website</title>
    <link>/tags/neural-symbolic/</link>
    <description>Recent content in Neural-Symbolic on yo3nglau academic website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/neural-symbolic/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(arXiv 22&#39;) SymFormer- End-to-end symbolic regression using transformer-based architecture</title>
      <link>/post/arxiv-22-rgb-no-more-minimally-decoded-jpeg-vision-transformers/</link>
      <pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/arxiv-22-rgb-no-more-minimally-decoded-jpeg-vision-transformers/</guid>
      <description>Overview Motivation Many real-world problems can be naturally described by mathematical formulas. The task of finding formulas from a set of observed inputs and outputs is called symbolic regression.
Neural networks have been applied to symbolic regression, among which the transformer-based ones seem to be the most promising. However, the main drawback of transformers is that they generate formulas without numerical constants, which have to be optimized separately, so yielding suboptimal results.</description>
    </item>
    
    <item>
      <title>(arXiv 22&#39;) SymFormer- End-to-end symbolic regression using transformer-based architecture</title>
      <link>/post/cvpr-23-symformer-end-to-end-symbolic-regression-using-transformer-based-architecture-copy/</link>
      <pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/cvpr-23-symformer-end-to-end-symbolic-regression-using-transformer-based-architecture-copy/</guid>
      <description>Overview Motivation Many real-world problems can be naturally described by mathematical formulas. The task of finding formulas from a set of observed inputs and outputs is called symbolic regression.
Neural networks have been applied to symbolic regression, among which the transformer-based ones seem to be the most promising. However, the main drawback of transformers is that they generate formulas without numerical constants, which have to be optimized separately, so yielding suboptimal results.</description>
    </item>
    
    <item>
      <title>(ICCV 21&#39;) Calibrating Concepts and Operations- Towards Symbolic Reasoning on Real Images</title>
      <link>/post/iccv-21-calibrating-concepts-and-operations-towards-symbolic-reasoning-on-real-images/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/iccv-21-calibrating-concepts-and-operations-towards-symbolic-reasoning-on-real-images/</guid>
      <description>Overview &amp;ldquo;Figure 1: Statistics and examples from the synthetic CLEVR dataset and the real GQA dataset. Compared to the synthetic dataset, VQA on real data needs to deal with long-tail concept distribution and uneven importance of reasoning steps.&amp;rdquo;
Motivation While neural symbolic methods demonstrate impressive performance in visual question answering on synthetic images, their performance suffers on real images. This work identifies that the long-tail distribution of visual concepts and unequal importance of reasoning steps in real data are the two key obstacles that limit the models’ real-world potentials.</description>
    </item>
    
    <item>
      <title>(ICLR 19&#39; oral) THE NEURO-SYMBOLIC CONCEPT LEARNER- INTERPRETING SCENES, WORDS, AND SENTENCES FROM NATURAL SUPERVISION</title>
      <link>/post/iclr-19-oral-the-neuro-symbolic-concept-learner-interpreting-scenes-words-and-sentences-from-natural-supervision/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/iclr-19-oral-the-neuro-symbolic-concept-learner-interpreting-scenes-words-and-sentences-from-natural-supervision/</guid>
      <description>Overview &amp;ldquo;Figure 7: An example image-question pair from the VQS dataset and the corresponding execution trace of NS-CL.&amp;rdquo;
This work proposes the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, the model learns by simply looking at images and reading paired questions and answers. The proposed model builds an object-based scene representation and translates sentences into executable, symbolic programs.</description>
    </item>
    
    <item>
      <title>(NeurIPS 22&#39;) End-to-end symbolic regression with transformers</title>
      <link>/post/neurips-22-end-to-end-symbolic-regression-with-transformers/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/neurips-22-end-to-end-symbolic-regression-with-transformers/</guid>
      <description>Overview Framework Unfamiliar knowledge Note: May include buzz word and potential literature.
Resources Paper, Code
Elementary drafts Note: May contain Chinese. This section will disappear once all drafts are embellished.
collapsed contentssomething一些草稿</description>
    </item>
    
  </channel>
</rss>
