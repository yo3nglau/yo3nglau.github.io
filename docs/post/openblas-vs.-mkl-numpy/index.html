<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>OpenBLAS vs. MKL NumPy | yo3nglau academic website</title>
    <link rel="stylesheet" href='/css/style.css' />
    <link rel="stylesheet" href='/css/fonts.css' />
    <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/github.min.css" rel="stylesheet">
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">OpenBLAS vs. MKL NumPy</span></h1>
<h2 class="author">yo3nglau</h2>
<h2 class="date">2025/11/16</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/computer-technology">Computer Technology</a> 
  
  
  
  Tags: <a href="/tags/guide">Guide</a> <a href="/tags/numpy">NumPy</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
  <ul>
    <li><a href="#preface">Preface</a>
      <ul>
        <li><a href="#blas-and-lapack">BLAS and LAPACK</a></li>
      </ul>
    </li>
    <li><a href="#openblas-open-source-and-widely-compatible">OpenBLAS: Open-Source and Widely Compatible</a>
      <ul>
        <li><a href="#advantages">Advantages</a></li>
        <li><a href="#limitations">Limitations</a></li>
      </ul>
    </li>
    <li><a href="#mkl-intels-high-performance-option">MKL: Intel’s High-Performance Option</a>
      <ul>
        <li><a href="#advantages-1">Advantages</a></li>
        <li><a href="#limitations-1">Limitations</a></li>
      </ul>
    </li>
    <li><a href="#performance-comparison">Performance Comparison</a></li>
    <li><a href="#deployment">Deployment</a></li>
    <li><a href="#experiments">Experiments</a>
      <ul>
        <li><a href="#cpu-information">CPU Information</a></li>
        <li><a href="#benchmark">Benchmark</a></li>
        <li><a href="#performance-highlights">Performance Highlights</a></li>
      </ul>
    </li>
    <li><a href="#choice">Choice</a>
      <ul>
        <li><a href="#choose-openblas-if">Choose OpenBLAS if:</a></li>
        <li><a href="#choose-mkl-if">Choose MKL if:</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#resources">Resources</a></li>
  </ul>
</nav>


<main>
<h2 id="preface">Preface</h2>
<p>When working with scientific computing or machine learning in Python, NumPy is one of the core dependencies for numerical operations. Behind the scenes, NumPy relies on optimized linear algebra libraries—primarily <strong>OpenBLAS</strong> or <strong>Intel MKL (Math Kernel Library)</strong>—to accelerate matrix multiplications, convolutions, decompositions, and other dense numerical routines.
This article provides a clear comparison between the OpenBLAS-based and MKL-based NumPy distributions, helping you make an informed choice for your development environment.</p>
<h3 id="blas-and-lapack">BLAS and LAPACK</h3>
<p>NumPy delegates its heavy numerical computation to low-level libraries such as <strong>BLAS</strong> (Basic Linear Algebra Subprograms) and <strong>LAPACK</strong> (Linear Algebra PACKage). These libraries provide optimized implementations of common mathematical operations.</p>
<ul>
<li><strong>BLAS</strong>: Vector and matrix operations</li>
<li><strong>LAPACK</strong>: Matrix factorizations and solvers built on top of BLAS</li>
</ul>
<p>The performance of NumPy often depends more on the BLAS/LAPACK backend than on NumPy itself.</p>
<h2 id="openblas-open-source-and-widely-compatible">OpenBLAS: Open-Source and Widely Compatible</h2>
<p><strong>OpenBLAS</strong> is an open-source, actively maintained implementation of BLAS and LAPACK. It is the default backend for many community-driven builds (e.g., Linux distributions, Python from source).</p>
<h3 id="advantages">Advantages</h3>
<ul>
<li><strong>Open-source &amp; community-supported</strong>
No vendor lock-in and easy to distribute.</li>
<li><strong>Good performance across architectures</strong>
Particularly optimized for x86, ARM, and PowerPC.</li>
<li><strong>Lightweight and easy to build</strong>
Ideal for environments requiring portability.</li>
</ul>
<h3 id="limitations">Limitations</h3>
<ul>
<li><strong>Not always the fastest for Intel CPUs</strong>
Its optimizations may trail behind machine-specific tuning.</li>
<li><strong>Threading behavior can be inconsistent</strong>
Some workloads exhibit oversubscription or suboptimal thread usage.</li>
</ul>
<h2 id="mkl-intels-high-performance-option">MKL: Intel’s High-Performance Option</h2>
<p><strong>MKL</strong> (Math Kernel Library) is Intel’s proprietary, highly optimized numerical library. Anaconda’s NumPy distribution, for example, is compiled against MKL.</p>
<h3 id="advantages-1">Advantages</h3>
<ul>
<li><strong>Excellent performance on Intel hardware</strong>
MKL includes CPU-specific optimizations and vectorization.</li>
<li><strong>Consistent multi-threading</strong>
Thread scheduling is generally more stable and predictable.</li>
<li><strong>Broad functionality</strong>
Includes FFTs, sparse solvers, and advanced math routines beyond BLAS/LAPACK.</li>
</ul>
<h3 id="limitations-1">Limitations</h3>
<ul>
<li><strong>Not fully open-source</strong>
Redistribution and compilation flexibility are limited.</li>
<li><strong>Performance</strong> may vary on non-Intel CPUs
MKL can fall back to less optimized code paths (e.g., “generic” mode).</li>
</ul>
<h2 id="performance-comparison">Performance Comparison</h2>
<p>Although performance depends heavily on workload and hardware, the general trends are:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Workload Category</th>
          <th style="text-align: center">OpenBLAS</th>
          <th style="text-align: center">MKL</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">Dense matrix multiplication</td>
          <td style="text-align: center">Good</td>
          <td style="text-align: center">Excellent</td>
      </tr>
      <tr>
          <td style="text-align: center">Large eigenvalue/SVD computations</td>
          <td style="text-align: center">Good</td>
          <td style="text-align: center">Excellent</td>
      </tr>
      <tr>
          <td style="text-align: center">Multi-threaded workloads</td>
          <td style="text-align: center">Variable</td>
          <td style="text-align: center">Stable and fast</td>
      </tr>
      <tr>
          <td style="text-align: center">ARM-based devices (e.g., Raspberry Pi)</td>
          <td style="text-align: center">Often better</td>
          <td style="text-align: center">Unsupported</td>
      </tr>
      <tr>
          <td style="text-align: center">Cross-platform portability</td>
          <td style="text-align: center">Strong</td>
          <td style="text-align: center">Limited outside x86</td>
      </tr>
  </tbody>
</table>
<p>If your workflow includes heavy numerical operations (e.g., ML preprocessing, scientific simulations), MKL usually delivers better performance on Intel systems.</p>
<h2 id="deployment">Deployment</h2>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Package Manager</th>
          <th style="text-align: center">Default BLAS</th>
          <th style="text-align: center">OpenBLAS Option</th>
          <th style="text-align: center">MKL Option</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><strong>pip</strong></td>
          <td style="text-align: center">OpenBLAS</td>
          <td style="text-align: center">✔︎ default</td>
          <td style="text-align: center">✖ no official MKL wheel</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>uv</strong></td>
          <td style="text-align: center">OpenBLAS</td>
          <td style="text-align: center">✔︎ default</td>
          <td style="text-align: center">✖ same limitation as pip</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>conda (Anaconda)</strong></td>
          <td style="text-align: center">MKL</td>
          <td style="text-align: center">via <code>conda-forge</code></td>
          <td style="text-align: center">✔︎ default</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>conda-forge</strong></td>
          <td style="text-align: center">OpenBLAS</td>
          <td style="text-align: center">✔︎ default</td>
          <td style="text-align: center">✖ not provided</td>
      </tr>
  </tbody>
</table>
<p><a href="https://github.com/urob/numpy-mkl">urob/numpy-mkl</a> provides binary wheels for NumPy and SciPy, linked to Intel&rsquo;s high-performance <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html">oneAPI Math Kernel Library</a> for Intel CPUs.</p>
<h2 id="experiments">Experiments</h2>
<h3 id="cpu-information">CPU Information</h3>
<p>CPU: Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz</p>
<p>Physical cores: 64</p>
<p>Logical processors: 128</p>
<h3 id="benchmark">Benchmark</h3>
<pre><code class="language-python">import numpy as np, time, platform

N        = 8000  # 16000 24000 32000
DTYPE    = np.float64
REPEAT   = 7
WARMUP   = 2

def show_blas():
    np.__config__.show()

def benchmark():
    A = np.random.randn(N, N).astype(DTYPE)
    B = np.random.randn(N, N).astype(DTYPE)

    # warm up
    for _ in range(WARMUP):
        _ = np.dot(A, B)

    times = []
    for _ in range(REPEAT):
        t0 = time.perf_counter()
        C = np.dot(A, B)
        t1 = time.perf_counter()
        times.append(t1 - t0)
    best = np.median(times)
    gflops = 2 * N**3 / best * 1e-9
    return best, gflops, times


if __name__ == '__main__':
    print('Platform :', platform.processor())
    show_blas()
    dt, gflops, all_t = benchmark()
    print(f'N: {N}')
    print(f'Raw times (s): {[f&quot;{t:.3f}&quot; for t in all_t]}')
    print(f'Median  : {dt:.3f} s   ≈ {gflops:.1f} GFlops')
</code></pre>
<h3 id="performance-highlights">Performance Highlights</h3>
<h4 id="openblas-vs-mkl">OpenBLAS vs. MKL</h4>
<p><strong>Overall:</strong> MKL clearly outperforms OpenBLAS across all matrix sizes in both time and GFLOPS.</p>
<h5 id="speed">Speed</h5>
<p>MKL is <em>~20–35% faster</em> at N=8000 and maintains a strong advantage as N increases.
Example:</p>
<ul>
<li>N=8000 → MKL: <strong>0.582 s</strong>, OpenBLAS: <strong>0.725 s</strong></li>
<li>N=32000 → MKL: <strong>29.133 s</strong>, OpenBLAS: <strong>37.198 s</strong></li>
</ul>
<h5 id="throughput-gflops">Throughput (GFLOPS)</h5>
<p>MKL delivers consistently higher throughput, with about <strong>300–600 GFLOPS advantage</strong> across sizes.</p>
<ul>
<li>N=8000 → MKL: <strong>1758 GFLOPS</strong>, OpenBLAS: <strong>1413 GFLOPS</strong></li>
<li>N=24000 → MKL: <strong>2362 GFLOPS</strong>, OpenBLAS: <strong>1698 GFLOPS</strong></li>
</ul>
<h4 id="mkl-vs-urob-mkl">MKL vs. urob MKL</h4>
<p><strong>Overall:</strong> urob MKL provides <strong>slight improvements at small matrix sizes</strong> and <strong>performance very close to standard MKL</strong> overall.</p>
<h5 id="speed-1">Speed</h5>
<p>urob MKL is faster at smaller N and comparable at larger N:</p>
<ul>
<li>N=8000 → MKL: <strong>0.582 s</strong>, urob MKL: <strong>0.500 s</strong></li>
<li>N=16000 → MKL: <strong>3.969 s</strong>, urob MKL: <strong>3.995 s</strong> (nearly identical)</li>
<li>N=32000 → MKL: <strong>29.133 s</strong>, urob MKL: <strong>29.992 s</strong> (small difference)</li>
</ul>
<h5 id="throughput-gflops-1">Throughput (GFLOPS)</h5>
<p>urob MKL shows a <strong>small advantage</strong> in GFLOPS at smaller sizes but converges toward MKL at larger sizes:</p>
<ul>
<li>N=8000 → urob MKL: <strong>2048 GFLOPS</strong>, MKL: <strong>1758 GFLOPS</strong></li>
<li>N=24000 → urob MKL: <strong>2315 GFLOPS</strong>, MKL: <strong>2362 GFLOPS</strong></li>
</ul>
<img src="https://s21.ax1x.com/2025/11/16/pZPvVTU.png" style="zoom:30%;" />
<center>Figure 1: OpenBLAS vs. MKL</center>
<img src="https://s21.ax1x.com/2025/11/16/pZPvekF.png" style="zoom:30%;" />
<center>Figure 2: MKL vs. urob MKL</center>
<h2 id="choice">Choice</h2>
<h3 id="choose-openblas-if">Choose OpenBLAS if:</h3>
<ul>
<li>You need <strong>open-source</strong> and <strong>broad platform compatibility</strong></li>
<li>You work on <strong>ARM</strong>, <strong>non-Intel CPUs</strong>, or minimal Linux environments</li>
<li>You prefer lighter deployments and simpler distribution</li>
</ul>
<h3 id="choose-mkl-if">Choose MKL if:</h3>
<ul>
<li>You use an <strong>Intel CPU</strong> and prioritize the <strong>best performance</strong></li>
<li>You work in <strong>data science</strong>, <strong>AI</strong>, or <strong>heavy numerical computation</strong></li>
<li>You use <strong>Conda</strong>, where MKL integration is seamless</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>Both OpenBLAS and MKL deliver powerful numerical acceleration for NumPy, but they shine in different scenarios.
OpenBLAS provides portability, open-source accessibility, and solid performance across architectures.
MKL, on the other hand, offers top-tier, hardware-tuned performance that excels in multi-threaded and computationally intensive workloads.</p>
<p>Understanding these differences allows you to choose the right NumPy distribution for your environment—whether you&rsquo;re optimizing for speed, compatibility, or reproducibility.</p>
<h2 id="resources">Resources</h2>
<p><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html">Intel/MKL</a></p>
<p><a href="https://github.com/urob/numpy-mkl">urob/numpy-mkl</a></p>

</main>

  <footer>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/languages/r.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>
  
  <hr/>
  © <a href="https://github.com/yo3nglau">yo3nglau</a> 2025 | <a href="https://gohugo.io/">Hugo</a> | <a href="https://github.com/yihui/hugo-xmin">XMin</a>
  
  </footer>
  </body>
</html>

