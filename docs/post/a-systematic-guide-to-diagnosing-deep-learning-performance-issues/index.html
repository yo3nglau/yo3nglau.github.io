<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A Systematic Guide to Diagnosing Deep Learning Performance Issues | yo3nglau academic website</title>
    <link rel="stylesheet" href='/css/style.css' />
    <link rel="stylesheet" href='/css/fonts.css' />
    <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/github.min.css" rel="stylesheet">
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">A Systematic Guide to Diagnosing Deep Learning Performance Issues</span></h1>
<h2 class="author">yo3nglau</h2>
<h2 class="date">2025/11/07</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/computer-technology">Computer Technology</a> <a href="/categories/deep-learning">Deep Learning</a> 
  
  
  
  Tags: <a href="/tags/guide">guide</a> 
  
  
</p>
</div>



<main>
<h2 id="preface">Preface</h2>
<p>Training deep learning models can sometimes feel like waiting for water to boil—it tests your patience. When your model is training slower than expected, a systematic approach is key to identifying the bottleneck. Here is a step-by-step guide to diagnosing and fixing these performance issues.</p>
<h2 id="step-0-the-first-check---gpu-utilization"><strong>Step 0: The First Check - GPU Utilization</strong></h2>
<p>Your first command should always be to check if the GPU is even being used.</p>
<pre><code>nvidia-smi -l 1
</code></pre>
<p>This command provides a real-time view of your GPU&rsquo;s status. The critical metric is <code>GPU-Util</code>.</p>
<ul>
<li><strong>~0% Utilization:</strong> This is the most common culprit for beginners. It typically means your code isn&rsquo;t actually using the GPU. <strong>Fix:</strong> Ensure your model and data tensors are moved to the CUDA device (e.g., <code>.cuda()</code>or <code>.to(device)</code>).</li>
<li><strong>Sustained &gt;90% Utilization:</strong> Congratulations! Your GPU is the bottleneck. To speed things up, consider using multi-GPU training or a more powerful GPU.</li>
<li><strong>Low &amp; Fluctuating Utilization (e.g., peaks below 50%):</strong> This indicates a problem where the GPU is waiting for work. The bottleneck is likely elsewhere, often the CPU or data pipeline.</li>
</ul>
<h2 id="step-1-classify-your-training-task"><strong>Step 1: Classify Your Training Task</strong></h2>
<p>Understanding the nature of your task helps you know what to expect. The performance characteristics can be categorized as follows:</p>
<table>
  <thead>
      <tr>
          <th>Scenario</th>
          <th>CPU Usage</th>
          <th>GPU Usage</th>
          <th>Processing Time &amp; Fluctuation</th>
          <th>Typical Bottleneck</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1. Small Model, Simple Data</strong> (e.g., LeNet on MNIST)</td>
          <td>Low</td>
          <td>Low, but stable with little fluctuation</td>
          <td>Short and consistent</td>
          <td>The model itself is not computationally intensive. Little optimization room.</td>
      </tr>
      <tr>
          <td><strong>2. Small Model, Complex Data</strong> (e.g., ResNet-18 on ImageNet)</td>
          <td>High during data loading</td>
          <td>Low on average, with high, short peaks and long idle periods</td>
          <td>GPU time short, but cycle time long due to data loading</td>
          <td><strong>CPU (Data Preprocessing).</strong> The GPU finishes quickly and waits for the next batch.</td>
      </tr>
      <tr>
          <td><strong>3. Large Model, Simple Data</strong></td>
          <td>Low</td>
          <td>High and stable with little fluctuation</td>
          <td>Long but consistent</td>
          <td><strong>GPU (Model Computation).</strong> The GPU is constantly busy.</td>
      </tr>
      <tr>
          <td><strong>4. Large Model, Complex Data</strong></td>
          <td>High</td>
          <td>Variable, often unable to reach sustained high usage</td>
          <td>Long and potentially inconsistent</td>
          <td><strong>Mixed (CPU &amp; GPU).</strong> Both components are stressed and can become the bottleneck.</td>
      </tr>
  </tbody>
</table>
<p>For Scenarios 1 and 2, optimization focus should be on choosing the right hardware (better CPU for #2) and code structure. For Scenarios 3 and 4, if GPU usage is low, you need to dig deeper.</p>
<h2 id="step-2-check-cpu-utilization"><strong>Step 2: Check CPU Utilization</strong></h2>
<p>If your GPU usage is low and fluctuating (characteristic of Scenario 2 or 4), the CPU is the prime suspect.</p>
<ol>
<li>Check the CPU usage percentage (with <code>htop</code>, or <code>sysstat</code>). Note that 100% represents one core. If you has 5 cores, 500% usage means all cores are saturated.</li>
<li><strong>High CPU Usage (~100% * <code>core_count</code>):</strong> Your CPU is the bottleneck. <strong>Solution:</strong> Migrate your project to a host with more CPU cores.</li>
<li><strong>Low CPU Usage:</strong> Your code isn&rsquo;t efficiently leveraging the CPU for data loading. <strong>Solution:</strong> Increase the <code>num_workers</code>parameter in your PyTorch <code>DataLoader</code>. A good starting point is slightly less than the number of available CPU cores. Experiment to find the optimal value.</li>
</ol>
<h2 id="step-3-code-level-optimizations"><strong>Step 3: Code-Level Optimizations</strong></h2>
<p>If hardware isn&rsquo;t the clear issue, it&rsquo;s time to look at your code.</p>
<ul>
<li><strong>Avoid In-Iteration Overhead:</strong> Are you saving sample images or logging excessively every batch? These I/O operations block the training loop. Do them less frequently (e.g., every N epochs).</li>
<li><strong>Frequent Checkpointing:</strong> Saving the entire model can be slow. Reduce the frequency of saving model checkpoints.</li>
<li><strong>Use Official Guides:</strong> <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html">PyTorch</a> provides excellent performance tuning guides.</li>
</ul>
<hr>
<h2 id="common-pitfalls-and-advanced-tips">Common Pitfalls and Advanced Tips</h2>
<h3 id="1-the-pytorch-threading-issue-on-multi-card-instances"><strong>1. The PyTorch Threading Issue on Multi-Card Instances</strong></h3>
<p><strong>Symptoms:</strong> You rent a multi-GPU instance to run different experiments on different cards, but everything runs extremely slowly, with low utilization on both CPU and GPU.</p>
<p><strong>Root Cause:</strong> By default, PyTorch creates a number of threads equal to the number of CPU cores to accelerate operations. When multiple PyTorch processes (one per GPU experiment) each spawn this many threads, the system spends excessive time on thread scheduling instead of computation.</p>
<p><strong>Solution:</strong> Limit the number of threads per process by adding <code>torch.set_num_threads(N)</code>to your code, where <code>N</code>is a number like 4 or 8, depending on your core count and the number of concurrent experiments.</p>
<h3 id="2-the-numpy-performance-trap-on-intel-cpus"><strong>2. The NumPy Performance Trap on Intel CPUs</strong></h3>
<p><strong>Symptoms:</strong> Extremely high CPU usage (all cores maxed out) but low GPU usage, specifically on Intel CPUs.</p>
<p><strong>Root Cause:</strong> NumPy uses a math library for acceleration. Intel CPUs achieve significantly higher performance with the <strong>Intel Math Kernel Library (MKL)</strong> than with the generic <strong>OpenBLAS</strong>. Some Conda mirrors (like Tsinghua&rsquo;s) default to installing the OpenBLAS version of NumPy.</p>
<p><strong>Solution:</strong></p>
<pre><code># 1. Uninstall current numpy
pip uninstall numpy
# or: conda uninstall numpy

# 2. Temporarily remove the Conda mirror to get the default channel
echo &quot;&quot; &gt; /root/.condarc

# 3. Reinstall numpy, which will now fetch the MKL-optimized version
conda install numpy
</code></pre>
<h4 id="3-pro-tips-for-better-performance"><strong>3. Pro Tips for Better Performance</strong></h4>
<ul>
<li><strong>Prefer DDP over DP:</strong> For PyTorch multi-GPU training, use <code>torch.nn.DistributedDataParallel</code>(DDP) instead of <code>torch.nn.DataParallel</code>(DP). The official docs state that DDP &ldquo;offers much better performance and scaling to multiple-GPUs.&rdquo;</li>
<li><strong>Update Your PyTorch Version:</strong> If you are using Ampere architecture GPUs (e.g., RTX 3090, A100), upgrading PyTorch versions can yield significant performance improvements.</li>
<li><strong>Isolate Heavy Experiments:</strong> For resource-intensive hyperparameter tuning, it&rsquo;s better to run each experiment in a separate instance on different host machines rather than running them all on the same host/instance to avoid resource contention.</li>
</ul>
<h2 id="summary">Summary</h2>
<p>By following this logical flow—from GPU monitoring to task classification, then to CPU and code inspection—you can efficiently pinpoint what&rsquo;s slowing down your training and take the correct action to get back to full speed.</p>

</main>

  <footer>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/languages/r.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>
  
  <hr/>
  © <a href="https://github.com/yo3nglau">yo3nglau</a> 2025 | <a href="https://gohugo.io/">Hugo</a> | <a href="https://github.com/yihui/hugo-xmin">XMin</a>
  
  </footer>
  </body>
</html>

